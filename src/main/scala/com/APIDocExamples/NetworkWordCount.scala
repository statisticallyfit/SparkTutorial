package com.APIDocExamples

/**
 *
 */
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 *
 * Usage: NetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * STEPS to run this file:
 * 	1. cmd line: start netcat by typing: nc -lk 9999
 * 	2. idea: provide arguments via IDEA (Edit configurations) and they are: "localhost 9999"
 * 	3. idea: run this file
 * 	4. cmd line: provide streaming input data by typing lines then ENTER
 * 	5. idea repl: see how the words get counted (in tuple)
 *
 * SOURCE = https://hyp.is/1QaYBpuCEe2lec-iheRWyw/spark.apache.org/docs/latest/streaming-programming-guide.html
 */
object NetworkWordCount {
	def main(args: Array[String]): Unit = {
		if (args.length < 2) {
			System.err.println("Usage: NetworkWordCount <hostname> <port>")
			System.exit(1)
		}

		//StreamingExamples.setStreamingLogLevels()

		// Create the context with a 1 second batch size
		val sparkConf: SparkConf = new SparkConf().setMaster("local[3]").setAppName("NetworkWordCount")
		val ssc: StreamingContext = new StreamingContext(sparkConf, Seconds(1))

		// Create a socket stream on target ip:port and count the
		// words in input stream of \n delimited text (e.g. generated by 'nc')
		// Note that no duplication in storage level only for running locally.
		// Replication necessary in distributed scenario for fault tolerance.
		val lines: ReceiverInputDStream[String] = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
		val words: DStream[String] = lines.flatMap(_.split(" "))
		val wordCounts: DStream[(String, Int)] = words.map(x => (x, 1)).reduceByKey(_ + _)
		wordCounts.print()
		ssc.start()
		ssc.awaitTermination()
	}
}