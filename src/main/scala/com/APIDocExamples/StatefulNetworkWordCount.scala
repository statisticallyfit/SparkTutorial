package com.APIDocExamples

/**
 *
 */

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming._
import org.apache.spark.streaming.dstream.{DStream, MapWithStateDStream, ReceiverInputDStream}

// Tutorial source = https://github.com/apache/spark/blob/v3.3.1/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala

/**
 * Counts words cumulatively in UTF8 encoded, '\n' delimited text received from the network every
 * second starting with initial value of word count.
 * Usage: StatefulNetworkWordCount <hostname> <port>
 *   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
 *   data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example
 *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`
 */

// TODO - not begun yet

object StatefulNetworkWordCount {
	def main(args: Array[String]): Unit = {
		if (args.length < 2) {
			System.err.println("Usage: StatefulNetworkWordCount <hostname> <port>")
			System.exit(1)
		}

		//StreamingExamples.setStreamingLogLevels()

		val sparkConf: SparkConf = new SparkConf().setAppName("StatefulNetworkWordCount")
		// Create the context with a 1 second batch size
		val ssc: StreamingContext = new StreamingContext(sparkConf, Seconds(1))

		val PATH: String = "/development/projects/statisticallyfit/github/learningspark/SparkTutorial/src/main" +
			"/scala/com/APIDocExamples"
		//ssc.checkpoint(".")
		ssc.checkpoint(directory = PATH)

		// Initial state RDD for mapWithState operation
		val initialRDD: RDD[(String, Int)] = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))

		// Create a ReceiverInputDStream on target ip:port and count the
		// words in input stream of \n delimited test (e.g. generated by 'nc')
		val lines: ReceiverInputDStream[String] = ssc.socketTextStream(args(0), args(1).toInt)
		val words: DStream[String] = lines.flatMap(_.split(" "))
		val wordDstream: DStream[(String, Int)] = words.map(x => (x, 1))

		// Update the cumulative count using mapWithState
		// This will give a DStream made of state (which is the cumulative count of the words)
		val mappingFunc: (String, Option[Int], State[Int]) => (String, Int) = (word: String, one: Option[Int],
																  state: State[Int]) => {
			val sum: Int = one.getOrElse(0) + state.getOption.getOrElse(0)
			val output: (String, Int) = (word, sum)
			state.update(sum)
			output
		}

		val stateDstream: MapWithStateDStream[String, Int, Int, (String, Int)] = wordDstream.mapWithState(
			StateSpec.function(mappingFunc).initialState(initialRDD)
		)

		stateDstream.print()

		ssc.start()
		ssc.awaitTermination()
	}
}